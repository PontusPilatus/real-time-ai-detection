{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ab574f4-7f2f-4080-b643-9e0ac0de7e88",
   "metadata": {},
   "source": [
    "### Importera nödvändiga bibliotek\n",
    "I denna cell importerar vi alla nödvändiga bibliotek för att kunna hantera bildbearbetning, realtidskamerainmatning, samt det grafiska gränssnittet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a576b9-bdd4-4b79-85a2-21eecf13629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "\n",
    "from collections import deque\n",
    "from tkinter import ttk, Label\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f455acc-e51b-4c6a-9573-e0f1ef65893a",
   "metadata": {},
   "source": [
    "### Ladda modeller och initiera konstanter\n",
    "Här laddar vi de förtränade modellerna för ålder, kön och känslodetektion samt definierar de olika kategorierna för känslor och deras respektive färger. Vi initierar också köer för att hålla förutsägelser över tid, vilket hjälper till att stabilisera resultaten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9ea3a45-9b88-4cae-8e24-a793fce94466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ladda de förtränade modellerna\n",
    "age_gender_model = load_model('./models/age_gender_detection_model.keras')\n",
    "emotion_model = load_model('./models/improved_emotion_detection_model.keras')\n",
    "\n",
    "# Emotion kategorier\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Färgkarta för emotioner (BGR-format)\n",
    "emotion_colors = {\n",
    "    'Angry': (0, 0, 255),\n",
    "    'Disgust': (0, 255, 0),\n",
    "    'Fear': (255, 0, 0),\n",
    "    'Happy': (0, 255, 255),\n",
    "    'Sad': (255, 255, 0),\n",
    "    'Surprise': (255, 0, 255),\n",
    "    'Neutral': (200, 200, 200)\n",
    "}\n",
    "\n",
    "# Initiera köer för att hålla förutsägelser över tid\n",
    "age_queue = deque(maxlen=10)\n",
    "gender_prob_queue = deque(maxlen=10)\n",
    "emotion_queue = deque(maxlen=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71aab8b-b265-4f90-b6ff-d40c7bbe64bb",
   "metadata": {},
   "source": [
    "### Definiera funktioner för prediktion av ålder, kön och känslor\n",
    "Dessa funktioner ansvarar för att ta en ansiktsbild som indata och använda modellerna för att förutsäga ålder, kön och känslor. Resultaten används senare för att visa informationen i realtid på videoströmmen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df92d9c-4330-432b-a8de-065e3c9ffe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_age(predicted_age):\n",
    "    if predicted_age < 6:\n",
    "        return \"0-5\"\n",
    "    elif predicted_age < 13:\n",
    "        return \"6-12\"\n",
    "    elif predicted_age < 19:\n",
    "        return \"13-18\"\n",
    "    elif predicted_age < 26:\n",
    "        return \"19-25\"\n",
    "    elif predicted_age < 36:\n",
    "        return \"26-35\"\n",
    "    elif predicted_age < 46:\n",
    "        return \"36-45\"\n",
    "    elif predicted_age < 56:\n",
    "        return \"46-55\"\n",
    "    elif predicted_age < 66:\n",
    "        return \"56-65\"\n",
    "    elif predicted_age < 76:\n",
    "        return \"66-75\"\n",
    "    elif predicted_age < 86:\n",
    "        return \"76-85\"\n",
    "    elif predicted_age < 101:\n",
    "        return \"86-100\"\n",
    "    else:\n",
    "        return \"100+\"\n",
    "\n",
    "def predict_age_gender(face):\n",
    "    face_gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face_gray = cv2.resize(face_gray, (64, 64))\n",
    "    face_gray = np.expand_dims(face_gray, axis=-1)\n",
    "    face_gray = np.expand_dims(face_gray, axis=0)\n",
    "\n",
    "    age_prediction, gender_prediction = age_gender_model.predict(face_gray)\n",
    "    predicted_age = age_prediction[0][0]\n",
    "    predicted_age_range = categorize_age(predicted_age)\n",
    "    \n",
    "    return predicted_age_range, gender_prediction[0][0]\n",
    "\n",
    "def predict_emotion(face):\n",
    "    face_gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face_gray = cv2.resize(face_gray, (48, 48))\n",
    "    face_gray = face_gray.astype('float32') / 255.0\n",
    "    face_gray = img_to_array(face_gray)\n",
    "    face_gray = np.expand_dims(face_gray, axis=0)\n",
    "\n",
    "    emotion_prediction = emotion_model.predict(face_gray)\n",
    "    emotion_label = emotion_labels[np.argmax(emotion_prediction)]\n",
    "    \n",
    "    return emotion_label\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c1344-1a06-402a-8667-c388073c13de",
   "metadata": {},
   "source": [
    "### Definiera funktion för att rita text med kantlinje och bakgrund\n",
    "Denna funktion används för att rita text på videoströmmen med både en kantlinje och en semi-transparent bakgrund. Detta hjälper till att göra texten mer läsbar oavsett vad som visas i bakgrunden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88475c7a-b25b-443d-a371-90f40b4158f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text_with_background(frame, text, position, font, font_scale, text_color, bg_color, thickness=2, padding=5, alpha=0.6):\n",
    "    x, y = position\n",
    "\n",
    "    # Få textstorlek\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "    \n",
    "    # Definiera rektangelens hörn\n",
    "    top_left = (x - padding, y - text_height - padding)\n",
    "    bottom_right = (x + text_width + padding, y + baseline + padding)\n",
    "    \n",
    "    # Rita den semi-transparenta rektangeln\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, top_left, bottom_right, bg_color, cv2.FILLED)\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "    \n",
    "    # Rita kantlinje\n",
    "    cv2.putText(frame, text, (x, y), font, font_scale, (0, 0, 0), thickness + 2, cv2.LINE_AA)\n",
    "    # Rita huvudtexten ovanpå\n",
    "    cv2.putText(frame, text, (x, y), font, font_scale, text_color, thickness, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef298c62-4c75-44df-b0e6-2656b506b478",
   "metadata": {},
   "source": [
    "### Starta webbkameran och utför prediktioner i realtid\r\n",
    "I denna cell startar vi webbkameran, fångar ansikten i videoströmmen och gör förutsägelser om ålder, kön och känslor. Resultaten visas i realtid direkt på videoströmmen.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f93b6358-6960-4d5f-a242-3985183d3060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n"
     ]
    }
   ],
   "source": [
    "# Starta webbkameran\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Could not open webcam\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Dynamiskt effekt för rektangel runt ansikte\n",
    "        scale = 1.2 \n",
    "        cv2.rectangle(frame, (int(x + w * (1 - scale) / 2), int(y + h * (1 - scale) / 2)), \n",
    "                     (int(x + w * (1 + scale) / 2), int(y + h * (1 + scale) / 2)), \n",
    "                     (255, 255, 255), 2)\n",
    "\n",
    "        # Förutsäg ålder och kön\n",
    "        predicted_age_range, gender_prob = predict_age_gender(face)\n",
    "        age_queue.append(int(predicted_age_range.split('-')[0]))\n",
    "        gender_prob_queue.append(gender_prob)\n",
    "\n",
    "        # Förutsäg emotion och lägg till i kön\n",
    "        predicted_emotion = predict_emotion(face)\n",
    "        emotion_queue.append(predicted_emotion)\n",
    "\n",
    "        # Beräkna stabiliserade resultat\n",
    "        stable_age_numeric = np.mean(age_queue)\n",
    "        stable_gender_prob = np.mean(gender_prob_queue)\n",
    "\n",
    "        # Omvandla tillbaka från numeriskt värde till kategori \n",
    "        stable_age_range = categorize_age(stable_age_numeric)\n",
    "        stable_gender_str = 'Man' if stable_gender_prob < 0.3 else 'Woman'  # Justerat tröskelvärde\n",
    "\n",
    "        # Stabilisering av emotion\n",
    "        stable_emotion = most_common(list(emotion_queue))\n",
    "\n",
    "        # Hämta färgen för den stabiliserade emotionen\n",
    "        color = emotion_colors.get(stable_emotion, (255, 255, 255))  # Default till vit om emotion inte hittas\n",
    "\n",
    "        # Justera positionerna för att undvika överlapp\n",
    "        y_offset = 30  # Första texten visas 30 pixlar över ansiktets övre kant\n",
    "        text_spacing = 40  # Avstånd mellan textblocken\n",
    "\n",
    "        # Visa resultaten med justerade positioner\n",
    "        draw_text_with_background(frame, f\"Emotion: {stable_emotion}\", (x, y - y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, (0, 0, 0), thickness=1)\n",
    "        draw_text_with_background(frame, f\"Age: {stable_age_range}\", (x, y - y_offset - text_spacing), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, (0, 0, 0), thickness=1)\n",
    "        draw_text_with_background(frame, f\"Gender: {stable_gender_str}\", (x, y - y_offset - 2 * text_spacing), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, (0, 0, 0), thickness=1)\n",
    "\n",
    "    # Lägg till instruktionstexten längst ner i fönstret\n",
    "    height, width, _ = frame.shape\n",
    "    draw_text_with_background(frame, \"Press 'Q' to quit\", (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), (0, 0, 0), thickness=1)\n",
    "\n",
    "    cv2.imshow('Age, Gender, and Emotion Detector', frame)\n",
    "\n",
    "    # Få tangentinmatning\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9adba7-16ad-4f90-8000-38f1adbf9501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
